{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39174cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f41027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import tensorflow_data_validation as tfdv\n",
    "print('TFDV version: {}'.format(tfdv.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT='YOUR PORJECT'\n",
    "GOOGLE_CLOUD_REGION='us-central1'\n",
    "GCS_BUCKET_NAME='YOUR BUCKET'\n",
    "# The data directory contains the tfrecords prepared in https://betterprogramming.pub/a-step-by-step-guide-to-train-a-model-on-google-clouds-vertex-ai-47faafae1330\n",
    "DATA_ROOT = 'DATA LOCATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'cifar10'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# The golden schema comes from a previous run.\n",
    "GOLDEN_SCHEMA = 'gs://{}/pipeline_root/{}/schema'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Metadata is only used for local processing.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fda229",
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'cifar10_tfx.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_BATCH_SIZE = 32\n",
    "_EPOCH = 15\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int) -> tf.data.Dataset:\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, shuffle=True, shuffle_buffer_size=1000, label_key='label'),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model() -> tf.keras.Model:\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Reshape((32, 32, 3), input_shape=(3072,)))\n",
    "  model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dropout(0.4))\n",
    "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  schema = tfdv.load_schema_text(input_path=fn_args.schema_path)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema=schema,\n",
    "      batch_size=_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema=schema,\n",
    "      batch_size=_BATCH_SIZE)\n",
    "\n",
    "  model = _make_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      epochs=_EPOCH,\n",
    "      steps_per_epoch=100,\n",
    "      validation_steps=20,\n",
    "      validation_data=eval_dataset)\n",
    "\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b80c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str, golden_schema: str, metadata_path: str=''\n",
    "                     ) -> tfx.v1.dsl.Pipeline:\n",
    "  example_gen = tfx.components.ImportExampleGen(input_base=data_root,\n",
    "                                               input_config=tfx.proto.example_gen_pb2.Input(splits=[\n",
    "                                                   tfx.proto.example_gen_pb2.Input.Split(name='train', pattern='train.tfrecord'),\n",
    "                                                   tfx.proto.example_gen_pb2.Input.Split(name='eval', pattern='val.tfrecord')\n",
    "                                               ]))\n",
    "\n",
    "  statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "  schema_gen = tfx.components.SchemaGen(statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "  schema_importer = tfx.v1.dsl.Importer(source_uri=golden_schema,\n",
    "                                    artifact_type=tfx.types.standard_artifacts.Schema).with_id('schema_importer')\n",
    "\n",
    "  example_validator = tfx.components.ExampleValidator(statistics=statistics_gen.outputs['statistics'],\n",
    "                                                     schema=schema_importer.outputs['result'])\n",
    "\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      train_args=tfx.proto.trainer_pb2.TrainArgs(splits=['train']),\n",
    "      eval_args=tfx.proto.trainer_pb2.EvalArgs(splits=['eval']))\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      schema_gen,\n",
    "      schema_importer,\n",
    "      example_validator,\n",
    "      trainer,\n",
    "      #pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.v1.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      # Metadata config is only used for local processing.\n",
    "      #metadata_connection_config=tfx.orchestration.metadata\n",
    "      #                              .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process on Vertex AI\n",
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.v1.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.v1.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        golden_schema=GOLDEN_SCHEMA))\n",
    "\n",
    "from kfp.v2.google import client\n",
    "\n",
    "pipelines_client = client.AIPlatformClient(\n",
    "    project_id=GOOGLE_CLOUD_PROJECT,\n",
    "    region=GOOGLE_CLOUD_REGION,\n",
    ")\n",
    "\n",
    "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process locally.\n",
    "tfx.v1.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      golden_schema=GOLDEN_SCHEMA,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
